---
title: "nestedcv"
author: "Myles Lewis"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
    toc_depth: 3
    number_sections: false
fig_width: 6
vignette: >
  %\VignetteIndexEntry{nestedcv}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(nestedcv)
```

# Introduction

The motivation for this package is to provide functions which help with the
development and tuning of machine learning models in biomedical data where the
sample size if frequently limited, but the number of predictors may be
significantly larger (P >> n). While most machine learning pipelines involve
splitting data into training and testing cohorts, typically 2/3 and 1/3
respectively. Medical datasets may be too small for this, and so determination
of accuracy in the left-out test set suffers because the test set is small.
Nested cross-validation (CV) provides a way to get round this, by maximising use
of the whole dataset for testing overall accuracy, while maintaining the split
between training and testing.

In addition typical biomedical datasets often have many 10,000s of possible
predictors, so filtering of predictors is commonly needed. However, it has been
demonstrated that filtering on the whole dataset creates a bias when determining
accuracy of model (Vabalas et al, 2019). Thus, it is recommended that any
filtering of predictors is performed within the CV loops, to prevent data
leakage.

This package enables nested cross-validation (CV) to be performed using the
commonly used `glmnet` package, which fits elastic net regression models, and
the `caret` package, which is a general framework for fitting a large number of
machine learning models. In addition, `nestedcv` adds functionality to enable
cross-validation of the elastic net alpha parameter when fitting `glmnet`
models.

`nestedcv` partitions the dataset into outer and inner folds (default 10x10
folds). The inner fold CV, (default is 10-fold), is used to tune optimal
hyperparameters for models. Then the model is fitted on the whole inner fold and
tested on the left-out data from the outer fold. This repeated across all outer
folds (default 10 outer folds), and the unseen test predictions from the outer
folds are compared against the true results for the outer test folds and the
results concatenated, to give measures of accuracy (e.g. AUC and accuracy for
classification, or RMSE for regression) across the whole dataset.

Finally, the tuning parameters for each model in the outer folds are averaged to
give the mean best parameters across all outer folds. A final model is fitted
across the whole data using these final hyperparameters and can be used for
prediction with external data.

### Variable selection

While some models such as `glmnet` allow for sparsity and have variable
selection built-in, many models fail to fit when given massive numbers of
predictors, or perform poorly due to overfitting without variable selection. In
addition, in medicine one of the goals of predictive modelling is commonly the
development of diagnostic or biomarker tests, for which reducing the number of
predictors is typically a practical necessity.

Several filter functions (t-test, Wilcoxon test, anova, Pearson/Spearman
correlation, random forest variable importance, and ReliefF from the `CORElearn`
package) for feature selection are provided, and can be embedded within the
outer loop of the nested CV.

# Installation

Install from Github (requires API token).

```{r eval = FALSE}
devtools::install_github("myles-lewis/nestedcv", auth_token = "API token...")
library(nestedcv)
```

# Examples

### Nested CV with glmnet

In the example below, RNA-Sequencing gene expression data from synovial biopsies
from patients with rheumatoid arthritis in the R4RA randomised clinical trial
(Humby et al, 2021) is used to predict clinical response to the biologic drug
rituximab. Treatment response is determined by a clinical measure, namely
Clinical Disease Activity Index (CDAI) 50% response, which has a binary outcome:
treatment success or failure (response or non-response). This dataset contains
gene expression on over 50,000 genes in arthritic synovial tissue from 133
individuals, who were randomised to two drugs (rituximab and tocilizumab).
First, we remove genes of low expression using a median cut-off (this still
leaves >16,000 genes), and we subset the dataset to the rituximab treated
individuals (n=68).

```{r eval = FALSE}
# Data for this example can be downloaded from:
# DOI: 10.6084/m9.figshare.19336679

# set up data
load("/../R4RA_270821.RData")

index <- r4ra.meta$Outliers_Detected_On_PCA != "outlier" & r4ra.meta$Visit == 3 
          & !is.na(r4ra.meta$Visit)
metadata <- r4ra.meta[index, ]
dim(metadata)  # 133 individuals

medians <- Rfast::rowMedians(as.matrix(r4ra.vst))
data <- t(as.matrix(r4ra.vst))
# remove low expressed genes
data <- data[index, medians > 6]  
dim(data)  # 16254 genes

# Rituximab cohort only
yrtx <- metadata$CDAI.response.status.V7[metadata$Randomised.medication == "Rituximab"]
yrtx <- factor(yrtx)
data.rtx <- data[metadata$Randomised.medication == "Rituximab", ]

# no filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx,
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
res.rtx
```

<!-- previous output -->
```
## Nested cross-validation with glmnet
## No filter
## 
## Final parameters:
## lambda   alpha  
## 0.1511  0.7950  
## 
## Final coefficients:
## (Intercept)  AC016582.3       PCBP3    TMEM170B      EIF4E3     SEC14L6       CEP85        APLF 
##   0.8898659  -0.2676580  -0.2667770   0.2456329   0.2042326  -0.1992225   0.1076051  -0.1072684 
##       EARS2        PTK7       EFNA5        MEST      IQANK1    MTATP6P1       GSK3B       STK40 
##  -0.1036846  -0.0919594  -0.0882686   0.0769173  -0.0708992   0.0545392   0.0469272   0.0316988 
##     SUV39H2  AC005670.2      ZNF773        XIST       STAU2      DIRAS3 
##   0.0297370   0.0184851  -0.0170861  -0.0100934   0.0016182  -0.0009975 
## 
## Result:
##               AUC           Accuracy  Balanced accuracy  
##            0.7648             0.7059             0.6773
```

Use `summary()` to see full information from the nested model fitting. `coef()`
can be used to show the coefficients of the final fitted model.
Filters can be used by setting the `filterFUN` argument. Options for the filter 
function are passed as a list through `filter_options`.

```{r eval = FALSE}
# t-test filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, filterFUN = ttest_filter,
                         filter_options = list(nfilter = 300, p_cutoff = NULL),
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

Output from the nested CV with glmnet can be plotted to show how deviance is 
affected by alpha and lambda.

```{r eval = FALSE}
plot_alphas(res.rtx)
plot_lambdas(res.rtx)
```

```{r, out.width='100%', fig.align="center", echo=FALSE}
knitr::include_graphics("plot_alpha_lam.png")
```

The tuning of alpha for each outer fold can be plotted.

```{r eval = FALSE}
# Fold 1 line plot
plot(res.rtx$outer_result[[1]]$cvafit)

# Scatter plot
plot(res.rtx$outer_result[[1]]$cvafit, type = 'p')

# Number of non-zero coefficients
plot(res.rtx$outer_result[[1]]$cvafit, xaxis = 'nvar')
```

```{r, out.width='100%', fig.align="center", echo=FALSE}
knitr::include_graphics("plot_cva.png")
```

ROC curves from left-out folds from both outer and inner CV can be plotted. Note
that the AUC based on the left-out outer folds is the unbiased estimate of
accuracy, while the left-out inner folds demonstrate bias due to the
optimisation of the model's hyperparameters on the inner fold data.

```{r eval = FALSE}
# Outer CV ROC
plot(res.rtx$roc, main = "Outer fold ROC", font.main = 1, col = 'blue')
legend("bottomright", legend = paste0("AUC = ", signif(pROC::auc(res.rtx$roc), 3)), bty = 'n')

# Inner CV ROC
plot(rtx.inroc, main = "Inner fold ROC", font.main = 1, col = 'red')
legend("bottomright", legend = paste0("AUC = ", signif(pROC::auc(rtx.inroc), 3)), bty = 'n')
```

```{r, out.width='100%', fig.align="center", echo=FALSE}
knitr::include_graphics("roc.png")
```

The overall expression level of each gene selected in the final model can be 
compared with a boxplot.

```{r eval = FALSE}
boxplot_model(res.rtx, data.rtx, ylab = "VST")
```

```{r, out.width='70%', fig.align="center", echo=FALSE}
knitr::include_graphics("boxplot.png")
```

Other filters include Wilcoxon (Mann-Whitney) test, Pearson or Spearman 
correlation for regression modelling, random forest and ReliefF filters.

```{r eval = FALSE}
# Random forest filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0.5, filterFUN = rf_filter,
                         filter_options = list(nfilter = 300),
                         family = "binomial", cores = 8, 
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)

# ReliefF algorithm filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0, filterFUN = relieff_filter,
                         filter_options = list(nfilter = 300),
                         family = "binomial", cores = 8, 
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

Leave-one-out cross-validation (LOOCV) can be performed on the outer folds.

```{r eval = FALSE}
# Outer LOOCV
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0, filterFUN = ttest_filter,
                         filter_options = list(nfilter = 300, p_cutoff = NULL),
                         outer_method = "loocv",
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

### Nested CV with caret

Nested CV can also be performed using the caret package framework. This enables
access to the large library of machine learning models available within caret.

When fitting classification models, the usual default metric for tuning model
hyperparameters in `caret` is Accuracy. However, with small datasets, accuracy
is disproportionately affected by changes in a single individual's prediction
outcome from incorrect to correctly classified or vice versa. For this reason,
we suggest using logLoss with smaller datasets as it provides more stable
measures of model tuning behaviour. In `nestedcv`, when fitting classification
models with `caret`, the default metric is changed to use logLoss.

Here we use caret for tuning the alpha and lambda parameters of glmnet.

```{r eval = FALSE}
# nested CV using caret
tg <- expand.grid(lambda = exp(seq(log(2e-3), log(1e0), length.out = 100)),
                  alpha = seq(0.8, 1, 0.1))
ncv <- nestcv.train(y = yrtx, x = data.rtx,
               method = "glmnet",
               savePredictions = "final",
               filterFUN = ttest_filter, filter_options = list(nfilter = 300),
               tuneGrid = tg, cores = 8)
ncv$summary

# Plot ROC on outer folds
plot(ncv$roc)

# Plot ROC on inner LO folds
inroc <- innercv_roc(ncv)
plot(inroc)
auc(inroc)

# Show example tuning plot for outer fold 1
plot(ncv$outer_result[[1]]$fit, xTrans = log)

# Extract coefficients of final fitted model
glmnet_coefs(ncv$final_fit$finalModel, s = ncv$finalTune$lambda)
```

# References

Humby F, Durez P, Buch MH, et al. Rituximab versus tocilizumab in anti-TNF
inadequate responder patients with rheumatoid arthritis (R4RA): 16-week outcomes
of a stratified, biopsy-driven, multicentre, open-label, phase 4 randomised
controlled trial. Lancet. 2021;397(10271):305-317.

Vabalas A, Gowen E, Poliakoff E, Casson AJ. Machine learning algorithm
validation with a limited sample size. PloS one. 2019;14(11):e0224365.

