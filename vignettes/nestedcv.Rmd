---
title: "nestedcv"
author: "Myles Lewis"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
    toc_depth: 3
    number_sections: true
fig_width: 6
vignette: >
  %\VignetteIndexEntry{nestedcv}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(nestedcv)
```

# Introduction

The motivation for this package is to provide functions which help with the
development and tuning of machine learning models in biomedical data where the
sample size if frequently limited, but the number of predictors may be
significantly larger (P >> n). While most machine learning pipelines involve
splitting data into training and testing cohorts, typically 2/3 and 1/3
respectively. Medical datasets may be too small for this, and so determination
of accuracy in the left-out test set suffers because the test set is small.
Nested cross-validation (CV) provides a way to get round this, by maximising use
of the whole dataset for testing overall accuracy, while maintaining the split
between training and testing.

In addition typical biomedical datasets often have many 10,000s of possible
predictors, so filtering of predictors is commonly needed. However, it has been
demonstrated that filtering on the whole dataset creates a bias when determining
accuracy of model (Vabalas et al, 2019). Thus, it is recommended that any
filtering of predictors is performed within the CV loops, to prevent data
leakage.

This package enables nested cross-validation (CV) to be performed using the
commonly used `glmnet` package, which fits elastic net regression models, and
the `caret` package, which is a general framework for fitting a large number of
machine learning models. In addition, `nestedcv` adds functionality to enable
cross-validation of the elastic net alpha parameter when fitting `glmnet`
models.

`nestedcv` partitions the dataset into outer and inner folds (default 10x10
folds). The inner fold CV, (default is 10-fold), is used to tune optimal
hyperparameters for models. Then the model is fitted on the whole inner fold and
tested on the left-out data from the outer fold. This repeated across all outer
folds (default 10 outer folds), and the unseen test predictions from the outer
folds are compared against the true results for the outer test folds and the
results concatenated, to give measures of accuracy (e.g. AUC and accuracy for
classification, or RMSE for regression) across the whole dataset.

Finally, the tuning parameters for each model in the outer folds are averaged to
give the mean best parameters across all outer folds. A final model is fitted
across the whole data using these final hyperparameters and can be used for
prediction with external data.

While some models such as `glmnet` allow for sparsity and have variable
selection built-in, many models overfit or become unmanageable without variable
selection. In addition, in medicine one of the goals of predictive modelling is
commonly the development of diagnostic or biomarker tests, for which reducing
the number of predictors is typically a practical necessity.

Several filter functions (t-test, Wilcoxon test, anova, Pearson/Spearman
correlation, random forest, and ReliefF from the `CORElearn` package) for
feature selection are provided, and can be embedded within the outer loop of the
nested CV.

# Installation

Install from Github (requires API token).

```{r eval = FALSE}
devtools::install_github("myles-lewis/nestedcv", auth_token = "API token...")
library(nestedcv)
```

# Nested CV with glmnet

Using R4RA data to predict CDAI 50% response to rituximab.

```{r eval = FALSE}
# Data for this example can be downloaded from:
# DOI: 10.6084/m9.figshare.19336679

# set up data
load("/../R4RA_270821.RData")

index <- r4ra.meta$Outliers_Detected_On_PCA != "outlier" & r4ra.meta$Visit == 3 
          & !is.na(r4ra.meta$Visit)
metadata <- r4ra.meta[index, ]
dim(metadata)  # 133 individuals

medians <- Rfast::rowMedians(as.matrix(r4ra.vst))
data <- t(as.matrix(r4ra.vst))
# remove low expressed genes
data <- data[index, medians > 6]  
dim(data)  # 16254 genes

# Rituximab cohort only
yrtx <- metadata$CDAI.response.status.V7[metadata$Randomised.medication == "Rituximab"]
yrtx <- factor(yrtx)
data.rtx <- data[metadata$Randomised.medication == "Rituximab", ]

# no filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0.5,
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
res.rtx
```

Use `summary()` to see full information from the nested model fitting. `coef()`
can be used to show the coefficients of the final fitted model.
Filters can be used by setting the `filterFUN` argument. Options for the filter 
function are passed as a list through `filter_options`.

```{r eval = FALSE}
# t-test filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0, filterFUN = ttest_filter,
                         filter_options = list(nfilter = 300, p_cutoff = NULL),
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

Output from the nested CV with glmnet can be plotted to show how deviance is 
affected by alpha and lambda.

```{r eval = FALSE}
plot_alphas(res.rtx)
plot_lambdas(res.rtx)
```

The tuning of alpha for each outer fold can be plotted.

```{r eval = FALSE}
plot(res.rtx$outer_result[[1]]$cvafit)

# scatter plot
plot(res.rtx$outer_result[[1]]$cvafit, type = 'p')

# number of non-zero coefficients
plot(res.rtx$outer_result[[1]]$cvafit, xaxis = 'nvar')
```

ROC curves from left-out folds from both outer and inner CV can be plotted.

```{r eval = FALSE}
# Outer CV ROC
plot(res.rtx$roc)

# Inner CV ROC
rtx.inroc <- innercv_roc(res.rtx)
plot(rtx.inroc)
pROC::auc(rtx.inroc)
```

The overall expression level of each gene selected in the final model can be 
compared with a boxplot.

```{r eval = FALSE}
boxplot_model(res.rtx, data.rtx, ylab = "VST")
```

Other filters include wilcoxon (Mann-Whitney) test, Pearson or Spearman 
correlation for regression modelling, random forest and ReliefF filters.

```{r eval = FALSE}
# random forest filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0.5, filterFUN = rf_filter,
                         filter_options = list(nfilter = 300),
                         family = "binomial", cores = 8, 
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)

# ReliefF algorithm filter
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0, filterFUN = relieff_filter,
                         filter_options = list(nfilter = 300),
                         family = "binomial", cores = 8, 
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

Leave-one-out cross-validation (LOOCV) can be performed on the outer folds.

```{r eval = FALSE}
# outer LOOCV
res.rtx <- nestcv.glmnet(y = yrtx, x = data.rtx, min_1se = 0, filterFUN = ttest_filter,
                         filter_options = list(nfilter = 300, p_cutoff = NULL),
                         outer_method = "loocv",
                         family = "binomial", cores = 8,
                         alphaSet = seq(0.7, 1, 0.05))
summary(res.rtx)
```

# Nested CV with caret

Nested CV can also be performed using the caret package framework. This enables
access to the large library of machine learning models available within caret.

When fitting classification models, the usual default metric for tuning model
hyperparameters in `caret` is Accuracy. However, with small datasets, accuracy
is disproportionately affected by changes in a single individual's prediction
outcome from incorrect to correctly classified or vice versa. For this reason,
we suggest using logLoss with smaller datasets as it provides more stable
measures of model tuning behaviour. In `nestedcv`, when fitting classification
models with `caret`, the default metric is changed to use logLoss.

Here we use caret for tuning the alpha and lambda parameters of glmnet.

```{r eval = FALSE}
# nested CV using caret
tg <- expand.grid(lambda = exp(seq(log(2e-3), log(1e0), length.out = 100)),
                  alpha = seq(0.8, 1, 0.1))
ncv <- nestcv.train(y = yrtx, x = data.rtx,
               method = "glmnet",
               savePredictions = "final",
               filterFUN = ttest_filter, filter_options = list(nfilter = 300),
               tuneGrid = tg, cores = 8)
ncv$summary

# Plot ROC on outer folds
plot(ncv$roc)

# Plot ROC on inner LO folds
inroc <- innercv_roc(ncv)
plot(inroc)
auc(inroc)

# Show example tuning plot for outer fold 1
plot(ncv$outer_result[[1]]$fit, xTrans = log)

# Extract coefficients of final fitted model
glmnet_coefs(ncv$final_fit$finalModel, s = ncv$finalTune$lambda)
```

# References

Vabalas A, Gowen E, Poliakoff E, Casson AJ. Machine learning algorithm
validation with a limited sample size. PloS one. 2019;14(11):e0224365.
