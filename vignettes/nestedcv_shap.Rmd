---
title: "Explaining nestedcv models with Shapley values"
author: "Myles Lewis"
output:
  html_document:
fig_width: 6
vignette: >
  %\VignetteIndexEntry{Explaining nestedcv models with Shapley values}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE
)
library(nestedcv)
```

`nestedcv` provides two methods for understanding fitted models. The simplest of
these is to plot variable importance. The newer method is to calculate Shapley
values for each predictor.

### Variable importance and variable stability

For regression model systems such as glmnet variable importance is represented
as the coefficients of the model scaled by absolute value from largest to
smallest. However, the outer folds of nested CV allow us to show the variance of
model coefficients across each outer fold plus the final model, and hence see
how stable the model is. We can also overlay how often predictors are selected
in each model to give a sense of the stability of predictor selection.

In the example below using the Boston housing dataset, a glmnet regression model
is fitted and the variable importance for predictors is shown based on the
coefficients in the final model and tuned models from 10 outer folds. `min_1se`
is set to 1, which is the equivalent of specifying `s = "lambda.1se"` with
`glmnet`, to encourage a more sparse model.

```{r}
# Boston housing dataset
library(mlbench)
data(BostonHousing2)
dat <- BostonHousing2
y <- dat$cmedv
x <- subset(dat, select = -c(cmedv, medv, town, chas))

# Fit a glmnet model using nested CV
set.seed(1, "L'Ecuyer-CMRG")
fit <- nestcv.glmnet(y, x, family = "gaussian",
                     min_1se = 1, alphaSet = 1, cv.cores = 2)
vs <- var_stability(fit)
vs
```

Variable stability can be plotted using `plot_var_stability()`.

```{r}
plot_var_stability(fit)
```

By default only the predictors chosen in the final model are shown. If the
argument `final` is set to `FALSE` all predictors are shown to help understand
how often they are selected which is helpful when pushing sparsity in models.
Original coefficients can be shown instead of being scaled as a percentage by
setting `percent = FALSE`.

We can overlay directionality for binary classification or regression models
using either colour (`direction = 1`) or the sign of the variable importance to
splay the plot (`direction = 2`).

```{r, fig.width = 9, fig.height = 5}
# overlay directionality using colour
p1 <- plot_var_stability(fit, final = FALSE, direction = 1)

# show directionality with the sign of the variable importance
p2 <- plot_var_stability(fit, final = FALSE, direction = 2)

ggpubr::ggarrange(p1, p2, ncol=2)
```

The `caret` package allows variable importance to be calculated for other models
types. This is model dependent. For example, with random forest variable
importance is usually calculated as the mean decrease in Gini impurity each time
a variable is chosen in a tree. With caret models, you may have to load the
appropriate package for that model to calculate the variable importance, e.g.
this is necessary for GBM models with the `gbm` package.

### Explainable AI with Shapley values

The original implementation of [shap](https://github.com/slundberg/shap) by
Scott Lundberg is a python package. We suggest using the R package
[fastshap](https://cran.r-project.org/package=fastshap) for examining `nestedcv`
models since it works with classification or regression as well as any model
type (regression such as glmnet or tree based such as random forest, GBM or
xgboost).

In the example below using the same glmnet regression model, the variable
importance for predictors is measured using Shapley values. The function
`explain()` from the `fastshap` package needs a wrapper function for prediction
using the model. `nestedcv` provides `pred_nestcv_glmnet` which is a wrapper
function for binary classification or regression with `nestcv.glmnet` fitted
models.

```{r}
library(fastshap)
library(ggplot2)

# Generate SHAP values using fastshap::explain
# Only using 5 repeats here for speed, but recommend higher values of nsim
sh <- explain(fit, X=x, pred_wrapper = pred_nestcv_glmnet, nsim = 5)

# Plot overall variable importance
plot_shap_bar(sh, x)
```

`nestedcv` also provides a quick plotting function `plot_shap_beeswarm` for
generating beeswarm plots similar to those made by the original python `shap`
package.

```{r}
# Plot beeswarm plot
plot_shap_beeswarm(sh, x, size = 1)
```

The process for a caret model fitted using `nestedcv` is similar. Use the
`pred_train` wrapper when calling `explain()`.

```{r}
# Only 3 outer folds to speed up process
fit <- nestcv.train(y, x,
                    method = "gbm",
                    n_outer_folds = 3, cv.cores = 2)

# Only using 5 repeats here for speed, but recommend higher values of nsim
sh <- explain(fit, X=x, pred_wrapper = pred_train, nsim = 5)
plot_shap_beeswarm(sh, x, size = 1)
```

For multinomial classification, a wrapper is needed for each class. For
`nestcv.glmnet` models, we provide wrappers for the first 3 classes:
`pred_nestcv_glmnet_class1`, `pred_nestcv_glmnet_class2` etc. They are very
simple functions and easy to extend to other classes.

For `nestcv.train()` models, similarly we provide `pred_train_class1`,
`pred_train_class2` etc for the first 3 classes. Again these are easily
extended.

As a toy example, we show the iris dataset which has 3 classes.

```{r, fig.width = 9, fig.height = 3.5}
data("iris")
dat <- iris
y <- dat$Species
x <- dat[, 1:4]

# Only 3 outer folds to speed up process
fit <- nestcv.glmnet(y, x, family = "multinomial", n_outer_folds = 3, alphaSet = 0.6)

# SHAP values for each of the 3 classes
sh1 <- explain(fit, X=x, pred_wrapper = pred_nestcv_glmnet_class1, nsim = 5)
sh2 <- explain(fit, X=x, pred_wrapper = pred_nestcv_glmnet_class2, nsim = 5)
sh3 <- explain(fit, X=x, pred_wrapper = pred_nestcv_glmnet_class3, nsim = 5)

s1 <- plot_shap_bar(sh1, x, sort = FALSE) +
  ggtitle("Setosa")
s2 <- plot_shap_bar(sh2, x, sort = FALSE) +
  ggtitle("Versicolor")
s3 <- plot_shap_bar(sh3, x, sort = FALSE) +
  ggtitle("Virginica")

ggpubr::ggarrange(s1, s2, s3, ncol=3, legend = "bottom", common.legend = TRUE)
```

Or using beeswarm plots.

```{r, fig.width = 9.5, fig.height = 3.5}
s1 <- plot_shap_beeswarm(sh1, x, sort = FALSE, cex = 0.7) +
  ggtitle("Setosa")
s2 <- plot_shap_beeswarm(sh2, x, sort = FALSE, cex = 0.7) +
  ggtitle("Versicolor")
s3 <- plot_shap_beeswarm(sh3, x, sort = FALSE, cex = 0.7) +
  ggtitle("Virginica")

ggpubr::ggarrange(s1, s2, s3, ncol=3, legend = "right", common.legend = TRUE)
```

### References

Lundberg SM & Lee SI (2017). A Unified Approach to Interpreting Model Predictions. *Advances in Neural Information Processing Systems* 30; 4768â€“4777. http://papers.nips.cc/ paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf

Lundberg SM et al (2020). From Local Explanations to Global Understanding with Explainable AI for Trees. *Nat Mach Intell* 2020; 2(1): 56-67. https://arxiv.org/abs/1905.04610

Software:
[fastshap](https://cran.r-project.org/package=fastshap): Brandon Greenwell. fastshap: Fast Approximate Shapley Values

