% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nestedcv.R
\name{nestcv.glmnet}
\alias{nestcv.glmnet}
\title{Nested cross-validation with glmnet}
\usage{
nestcv.glmnet(
  y,
  x,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  filterFUN = NULL,
  filter_options = NULL,
  n_outer_folds = 10,
  n_inner_folds = 10,
  alphaSet = seq(0, 1, 0.1),
  min_1se = 0,
  keep = TRUE,
  cores = 1,
  ...
)
}
\arguments{
\item{y}{Response vector}

\item{x}{Matrix of predictors}

\item{family}{Either a character string representing one of the built-in
families, or else a \code{glm()} family object. Passed to \link{cv.glmnet} and \link{glmnet}}

\item{filterFUN}{Filter function, e.g. \link{ttest_filter} or \link{relieff_filter}.
Any function can be provided and is passed \code{y} and \code{x}. Must return a
character vector with names of filtered predictors.}

\item{filter_options}{List of additional arguments passed to the filter
function specified by \code{filterFUN}.}

\item{n_outer_folds}{Number of outer CV folds}

\item{n_inner_folds}{Number of inner CV folds}

\item{alphaSet}{Vector of alphas to be tuned}

\item{min_1se}{Value from 0 to 1 specifying choice of optimal lambda from
0=lambda.min to 1=lambda.1se}

\item{keep}{Logical indicating whether inner CV predictions are
retained for calculating left-out inner CV fold accuracy etc. See argument
\code{keep} in \link{cv.glmnet}.}

\item{cores}{Number of cores for parallel processing. Note this currently
uses \link[parallel:mclapply]{parallel::mclapply}.}

\item{...}{Optional arguments passed to \link{cv.glmnet}}
}
\value{
An object with S3 class "nestcv.glmnet"
\item{output}{Predictions on the left-out outer folds}
\item{outer_result}{List object of results from each outer fold containing
predictions on left-out outer folds, best lambda, best alpha, fitted glmnet
coefficients, list object of inner fitted cv.glmnet and number of filtered
predictors at each fold.}
\item{outer_folds}{List of indices of outer training folds}
\item{mean_lambda}{Final mean best lambda from each fold}
\item{mean_alpha}{Final mean best alpha from each fold}
\item{final_fit}{Final fitted glmnet model}
\item{roc}{ROC AUC for binary classification where available.}
\item{summary}{Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for regression.}
}
\description{
Nested cross-validation (CV) with glmnet including tuning of elastic net
alpha parameter and embedding of a filter function within the nested CV.
}
