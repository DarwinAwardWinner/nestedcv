

#' Nested cross-validation for caret
#'
#' This function applies nested cross-validation (CV) to training of models
#' using the `caret` package. The function also allows the option of embedded
#' filtering of predictors for feature selection nested within the outer loop of
#' CV. Predictions on the outer test folds are brought back together and error
#' estimation/ accuracy determined. The default is 10x10 nested CV.
#'
#' @param y Response vector. For classification this should be a factor.
#' @param x Matrix of predictors
#' @param filterFUN Filter function, e.g. [ttest_filter] or [relieff_filter].
#'   Any function can be provided and is passed `y` and `x`. Must return a
#'   character vector with names of filtered predictors.
#' @param filter_options List of additional arguments passed to the filter
#'   function specified by `filterFUN`.
#' @param n_outer_folds Number of outer CV folds
#' @param metric A string that specifies what summary metric will be used to
#'   select the optimal model. By default, "logLoss" is used for classification
#'   and "RMSE" is used for regression. Note this differs from the default
#'   setting in caret which uses "Accuracy" for classification. See details.
#' @param trControl A list of values generated by the `caret` function
#'   [trainControl]. This defines how inner CV training through `caret` is
#'   performed. Default for the inner loop is 10-fold CV. See
#'   http://topepo.github.io/caret/using-your-own-model-in-train.html.
#' @param tuneGrid Data frame of tuning values, see [caret::train].
#' @param savePredictions Indicates whether hold-out predictions for each inner
#'   CV fold should be saved for ROC curves, accuracy etc see
#'   [caret::trainControl].Set to `"final"` to capture predictions for inner CV
#'   ROC.
#' @param cv.cores Number of cores for parallel processing. Note this currently
#'   uses `parallel::mclapply`.
#' @param ... Arguments passed to [caret::train]
#' @return An object with S3 class "nestcv.train"
#'   \item{call}{the matched call}
#'   \item{output}{Predictions on the left-out outer folds}
#'   \item{outer_result}{List object of results from each outer fold containing
#'   predictions on left-out outer folds, caret result and number of filtered
#'   predictors at each fold.}
#'   \item{dimx}{dimensions of `x`}
#'   \item{outer_folds}{List of indices of outer training folds}
#'   \item{final_fit}{Final fitted caret model using best tune parameters}
#'   \item{final_vars}{Column names of filtered predictors entering final model}
#'   \item{roc}{ROC AUC for binary classification where available.}
#'   \item{trControl}{`caret::trainControl` object used for inner CV}
#'   \item{bestTunes}{best tuned parameters from each outer fold}
#'   \item{finalTune}{final parameters used for final model}
#'   \item{summary}{Overall performance summary. Accuracy and balanced accuracy
#'   for classification. ROC AUC for binary classification. RMSE for
#'   regression.}
#' @details Parallelisation is performed on the outer folds using `mclapply`.
#'   For classification `metric` defaults to using 'logLoss' with the
#'   `trControl` arguments `classProbs = TRUE, summaryFunction = mnLogLoss`,
#'   rather than 'Accuracy' which is the default classification metric in
#'   `caret`. See [trainControl]. LogLoss is arguably more consistent than
#'   Accuracy for tuning parameters in datasets with small sample size.
#'
#'   Models can be fitted with a single set of fixed parameters, in which case
#'   `trControl` defaults to `trainControl(method = "none")` which disables
#'   inner CV as it is unnecessary. See
#'   https://topepo.github.io/caret/model-training-and-tuning.html#fitting-models-without-parameter-tuning
#'
#' @author Myles Lewis
#' @importFrom caret createFolds train trainControl mnLogLoss confusionMatrix
#'   defaultSummary
#' @importFrom data.table rbindlist
#' @importFrom parallel mclapply
#' @importFrom pROC roc
#' @importFrom stats predict setNames
#' @export
#' 
nestcv.train <- function(y, x,
                         filterFUN = NULL,
                         filter_options = NULL, 
                         n_outer_folds = 10,
                         cv.cores = 1,
                         metric = ifelse(is.factor(y), "logLoss", "RMSE"),
                         trControl = NULL,
                         tuneGrid = NULL,
                         savePredictions = FALSE,
                         ...) {
  nestcv.call <- match.call(expand.dots = TRUE)
  if (is.null(trControl)) {
    trControl <- if (is.factor(y)) {
      trainControl(method = "cv", 
                   number = 10,
                   classProbs = TRUE,
                   savePredictions = savePredictions,
                   summaryFunction = mnLogLoss)
    } else trainControl(method = "cv", 
                        number = 10,
                        savePredictions = savePredictions)
  }
  # switch off inner CV if tuneGrid is single row
  if (!is.null(tuneGrid)) {
    if (nrow(tuneGrid) == 1) trControl <- trainControl(method = "none", classProbs = TRUE)
  }
  outer_folds <- createFolds(y, k = n_outer_folds, returnTrain = TRUE)
  outer_res <- mclapply(1:n_outer_folds, function(i) {
    trainIndex <- outer_folds[[i]]
    filtx <- if (is.null(filterFUN)) x else {
      args <- list(y = y[trainIndex], x = x[trainIndex, ])
      args <- append(args, filter_options)
      fset <- do.call(filterFUN, args)
      x[, fset]
    }
    fit <- caret::train(x = filtx[trainIndex, ], y = y[trainIndex],
                        metric = metric,
                        trControl = trControl,
                        tuneGrid = tuneGrid, ...)
    predy <- predict(fit, newdata = filtx[-trainIndex, ])
    preds <- data.frame(predy=predy, testy=y[-trainIndex])
    if (is.factor(y)) {
      predyp <- predict(fit, newdata = filtx[-trainIndex, ], type = "prob")
      # note predyp has 2 columns
      preds$predyp <- predyp[,2]
    }
    rownames(preds) <- rownames(x[-trainIndex, , drop = FALSE])
    ret <- list(preds = preds,
                fit = fit,
                nfilter = ncol(filtx))
    ret
  }, mc.cores = cv.cores)
  predslist <- lapply(outer_res, '[[', 'preds')
  output <- data.table::rbindlist(predslist)
  output <- as.data.frame(output)
  if (!is.null(rownames(x))) {
    rownames(output) <- unlist(lapply(predslist, rownames))}
  summary <- predSummary(output)
  caret.roc <- NULL
  if (is.factor(y) & nlevels(y) == 2) {
    caret.roc <- pROC::roc(output$testy, output$predyp, direction = "<", 
                           quiet = TRUE)
  }
  bestTunes <- lapply(outer_res, function(i) i$fit$bestTune)
  bestTunes <- as.data.frame(data.table::rbindlist(bestTunes))
  rownames(bestTunes) <- paste('Fold', seq_len(n_outer_folds))
  finalTune <- colMeans(bestTunes)
  finalTune <- data.frame(as.list(finalTune))
  filtx <- if (is.null(filterFUN)) x else {
    args <- list(y = y, x = x)
    args <- append(args, filter_options)
    fset <- do.call(filterFUN, args)
    x[, fset]
  }
  fitControl <- trainControl(method = "none", classProbs = TRUE)
  final_fit <- caret::train(x = filtx, y = y, 
                            trControl = fitControl,
                            tuneGrid = finalTune, ...)
  
  out <- list(call = nestcv.call,
              output = output,
              outer_result = outer_res,
              dimx = dim(x),
              outer_folds = outer_folds,
              final_fit = final_fit,
              final_vars = colnames(filtx),
              roc = caret.roc,
              trControl = trControl,
              bestTunes = bestTunes,
              finalTune = finalTune,
              summary = summary)
  class(out) <- "nestcv.train"
  out
}

#' @export
summary.nestcv.train <- function(object, 
                                 digits = max(3L, getOption("digits") - 3L), 
                                 ...) {
  cat("Nested cross-validation with caret\n")
  if (!is.null(object$call$filterFUN)) 
    cat("Filter: ", object$call$filterFUN, "\n") else cat("No filter\n")
  cat("Outer loop: ", paste0(length(object$outer_folds), "-fold cv\n"))
  cat("Inner loop: ", paste0(object$trControl$number, "-fold ",
                             object$trControl$method, "\n"))
  cat(object$dimx[1], "observations,", object$dimx[2], "predictors\n\n")
  nfilter <- unlist(lapply(object$outer_result, '[[', 'nfilter'))
  foldres <- object$bestTunes
  foldres$n.filter <- nfilter
  print(foldres, digits = digits, print.gap = 2L)
  cat("\nFinal parameters:\n")
  print(object$finalTune, digits = digits, print.gap = 2L, row.names = FALSE)
  cat("\nResult:\n")
  print(object$summary, digits = digits, print.gap = 2L)
  out <- list(dimx = object$dimx, folds = foldres,
              final_param = object$finalTune, result = object$summary)
  invisible(out)
}


#' @method predict nestcv.train
#' @export
predict.nestcv.train <- function(object, newdata, ...) {
  if (any(!object$final_vars %in% colnames(newdata))) 
    stop("newdata is missing some predictors", call. = FALSE)
  predict(object$final_fit, newdata = newdata[, object$final_vars], ...)
}
